{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the environment and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q transformers\n",
    "!git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./transformers/examples/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:13<00:00, 14.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": beware of growing lemons\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"When life gives you lemons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:14<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Thankfully she thought for a moment and saw that his face was even more vibrant than that of the disc joker\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"Harry witnessed Professor McGonagall walking right past Peeves who \\\n",
    "was determinedly loosening a crystal chandelier and could have sworn he heard her \\\n",
    "tell the poltergeist out of the corner of her mouth, 'It unscrews the other way.’\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Description\n",
    "* Try different prefix inputs\n",
    "* Try different temperatures with the --temperature argument.\n",
    "* Advanced: Try a different model than GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT2 talks about GPT2\n",
    "Using GPT2 to complete a partial description of GPT2 by OpenAI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:12<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " releasing 100GB of GPT-2 data to allow your private information to be compromised\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"Our model, called GPT-2 (a successor to GPT), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:12<00:00, 16.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It achieves some unique features: for example, it does not treat \"obesity as a disease\" as a subjective issue, since that would lead to every problem being treated as a disease\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT2 tries one line jokes\n",
    "##### (Varying temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:12<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If anyone knew how to set a bar high, it was me\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"A perfectionist walked into a bar. Apparently, the bar wasn’t set high enough.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:11<00:00, 17.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The bartender asked if he could get a drink\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --temperature=0.5 \\\n",
    "    --prompt=\"A perfectionist walked into a bar. Apparently, the bar wasn’t set high enough.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:10<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"I'm sorry, but I'm not going to be here,\" he said\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --temperature=0.2 \\\n",
    "    --prompt=\"A perfectionist walked into a bar. Apparently, the bar wasn’t set high enough.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distil GPT2 (model variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1042301/1042301 [00:00<00:00, 7287280.89B/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 456318/456318 [00:00<00:00, 3772068.84B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 574/574 [00:00<00:00, 591821.66B/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 352833716/352833716 [00:24<00:00, 14176940.96B/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:07<00:00, 26.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " He waved off an arm and left\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=distilgpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --temperature=1.0 \\\n",
    "    --prompt=\"A perfectionist walked into a bar. Apparently, the bar wasn’t set high enough.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 798011/798011 [00:00<00:00, 5709700.62B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 641/641 [00:00<00:00, 322600.06B/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 467042463/467042463 [00:38<00:00, 12247447.16B/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:30<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They proceeded to sort out the problem, but a break in the floor left them bent over a table\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=xlnet \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=xlnet-base-cased \\\n",
    "    --stop_token=\".\" \\\n",
    "    --temperature=1.0 \\\n",
    "    --prompt=\"A perfectionist walked into a bar. Apparently, the bar wasn’t set high enough.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:30<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", sour lemons, good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, with good lemons, the day comes when you truly want to be a God\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=xlnet \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=xlnet-base-cased \\\n",
    "    --stop_token=\".\" \\\n",
    "    --temperature=1.0 \\\n",
    "    --prompt=\"When life gives you lemons \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 798011/798011 [00:00<00:00, 6406633.32B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 699/699 [00:00<00:00, 350444.48B/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 1441285815/1441285815 [01:45<00:00, 13666312.12B/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:53<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", lemonade is the last thing you want\n"
     ]
    }
   ],
   "source": [
    "%run ./transformers/examples/run_generation.py --model_type=xlnet \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=xlnet-large-cased\\\n",
    "    --stop_token=\".\" \\\n",
    "    --temperature=1.0 \\\n",
    "    --prompt=\"When life gives you lemons \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
