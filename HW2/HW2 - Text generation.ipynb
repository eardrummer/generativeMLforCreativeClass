{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Text Generative Models\n",
    "\n",
    "In this assignment we will see some generative models for text: CharRNN, Transformers and Chatbots. Training text models is very time consuming, and uses a ton of data. The really good models also tend to be very large, so we will stick to pretrained models. Those can still be excellent to generate totally new text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Embeddings are numeric representations for non-numeric data. In our case we look for embeddings for words. A simple kind of embedding is One-Hot Encoding, where we put a `1` in a vector of all `0`s at the index of the word in the vocabulary.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1\" width=\"50%\"/>\n",
    "\n",
    "But that can be very wasteful and also doesn't encode any relationship between the words.\n",
    "\n",
    "To learn semantic relationship a few unsupervised algorithms were proposed. In class we've discussed Continuous Bag of Words and Skip-Gram. Essentially these will mask out part of a sentence and ask the model to predict the missing part. This way the model learns about the context words are used in sentences as well as relationships.\n",
    "\n",
    "Embedding for a word is a vector of numbers:\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1\" width=\"50%\" />\n",
    "\n",
    "Luckily many of the world leaders in natural language processing have pretrained word embeddings learned on huge corpora, so we don't have to do it ourselves.\n",
    "\n",
    "Allison Parrish of NYU showed some very interesting uses for word embeddings for poetry generation: https://www.youtube.com/watch?v=L3D0JEA1Jdc breeze through this StrangeLoop talk for inspiration. I encourage you to try these methods towards you own generative work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chakin` is a helper tool for downloading pretrained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chakin progressbar2 textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chakin\n",
    "import progressbar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "chakin.search('English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download GLoVE embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100% ||                                      | Time:  0:06:32   2.1 MiB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./glove.6B.zip'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chakin.download(number=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need one file (the smallest dimension one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!unzip glove.6B.zip glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files contain the embedding values for each word in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!head -5 glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load them into memory and organize a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_lines = open('glove.6B/glove.6B.50d.txt','rt', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100000 of 100000) |################| Elapsed Time: 0:00:03 Time:  0:00:03\n"
     ]
    }
   ],
   "source": [
    "w2v_emb_dict = dict()\n",
    "pbar = progressbar.ProgressBar(max_value=100000)\n",
    "for i,l in enumerate(w2vec_lines[1:100000]):\n",
    "    w,emb = l.split(' ', 1)\n",
    "    w2v_emb_dict[w] = np.fromstring(emb, sep=' ')\n",
    "    pbar.update(i+1)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These would be the first most commonly used tokens in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(w2v_emb_dict.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies and Similarities\n",
    "\n",
    "Embeddings carry semantic information in their numeric encoding. Exploring this semantic space can be fun, for example looking for similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is measuring the angle between vectors. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2432/1*Acs3Kbrrrb4d3fqMlGhMcQ.png\"/>\n",
    "\n",
    "Our embeddings are normalized vectors so looking at the angle between two vectors reveals how far away they are from one another in the high-dimensional embdding space:\n",
    "\n",
    "<img src=\"https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "w2v_emb_dict_keys = list(w2v_emb_dict.keys())\n",
    "w2v_emb_dict_values = np.array(list(w2v_emb_dict.values()))\n",
    "\n",
    "def find_nearest(w):\n",
    "    return w2v_emb_dict_keys[cosine_similarity(w2v_emb_dict[w].reshape(1,-1), w2v_emb_dict_values)[0].argsort()[-2]]\n",
    "def find_nearest_top_k(v, k=5):\n",
    "    return [w2v_emb_dict_keys[w] for w in cosine_similarity(v.reshape(1,-1), w2v_emb_dict_values)[0].argsort()[-k:].tolist()[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at closest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bigger'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodbye'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teaching'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest('learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider \"**word analogies**\", e.g. completing the sentence: \"Paris is to France like Rome is to ___\" \\(Italy\\)\n",
    "\n",
    "To explain this geometrically:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2632/1*EOVxNmHkrsPQ7Q44N0OiQg.png\" width=\"60%\" />\n",
    "\n",
    "The offset vector between \"paris\" and \"france\" is the \"Captial of\" vector, and when we apply it to \"rome\" we expect to get \"italy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['italy', 'spain', 'rome', 'portugal', 'france']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['france'] - w2v_emb_dict['paris'] + w2v_emb_dict['rome'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king', 'queen', 'daughter', 'prince', 'throne']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['king'] - w2v_emb_dict['man'] + w2v_emb_dict['woman'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following analogies:\n",
    "1. sushi-rice is like pizza-___\n",
    "2. sushi-rice is like steak-___\n",
    "3. shirt-clothing is like phone-___\n",
    "4. shirt-clothing is like bowl-___\n",
    "5. book-reading is like TV-___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rice', 'bread', 'wheat', 'corn', 'with']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['rice'] - w2v_emb_dict['sushi'] + w2v_emb_dict['pizza'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rice', 'lamb', 'brown', 'curry', 'olive']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['rice'] - w2v_emb_dict['sushi'] + w2v_emb_dict['steak'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phone', 'customers', 'telephone', 'phones', 'cellular']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['clothing'] - w2v_emb_dict['shirt'] + w2v_emb_dict['phone'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bowl', 'ingredients', 'specialty', 'combine', 'foods']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['clothing'] - w2v_emb_dict['shirt'] + w2v_emb_dict['bowl'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tv', 'radio', 'television', 'broadcast', 'broadcasts']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['reading'] - w2v_emb_dict['book'] + w2v_emb_dict['tv'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find analogies that don't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woof', 'bbox', 'ugh', 'js04', 'absalom']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['woof'] - w2v_emb_dict['dog'] + w2v_emb_dict['cat'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'chicken', 'cat', 'duck', 'bite']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['dog'] - w2v_emb_dict['woof'] + w2v_emb_dict['meow'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leg', 'opener', 'warmup', 'match', 'tko']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['punch'] - w2v_emb_dict['hand'] + w2v_emb_dict['leg'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '4', '2', '5', '6']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['2'] - w2v_emb_dict['1'] + w2v_emb_dict['3'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intelligence', 'cia', 'fbi', 'officer', 'commanders']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['intelligence'] - w2v_emb_dict['artificial'] + w2v_emb_dict['real'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mrs', 'mr', 'mrs.', 'wife', 'daughter']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['mr'] - w2v_emb_dict['man'] + w2v_emb_dict['woman'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['electricity', 'fuel', 'kilowatts', 'megawatts', 'generating']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['electricity'] - w2v_emb_dict['edison'] + w2v_emb_dict['tesla'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deity', 'hindu', 'buddha', 'goddess', 'worshipping']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['jesus'] - w2v_emb_dict['christian'] + w2v_emb_dict['hindu'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought I'd test a very simple dual by testing the analogies between science and gender. And turns out, that the text that was used for pretraining  GloVe.6B.50d (Wikipedia+Gigaword 5 (6B vocab)) does have biases built into it from more examples of women with certain kinds of sciences (biology, psychology) and men with others (physice, economics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['science', 'physics', 'scientific', 'research', 'economics']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['science'] - w2v_emb_dict['woman'] + w2v_emb_dict['man'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['science', 'sciences', 'studies', 'biology', 'psychology']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_top_k(w2v_emb_dict['science'] - w2v_emb_dict['man'] + w2v_emb_dict['woman'], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char RNN\n",
    "\n",
    "CharRNN is a simple recurrent neural network architecture that works on the character level (not words). It's surprisingly powerful at generating text. These were popularized by [Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" width=\"50%\"/>\n",
    "\n",
    "The `textgenrnn` package is a convnient way to train and generate with CharRNNs. Here we're using its built in model. They have multiple models [published](https://github.com/minimaxir/textgenrnn/tree/master/weights) trained on different corpora.\n",
    "\n",
    "People created some very cool projects with it: https://github.com/minimaxir/textgenrnn#projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1029 12:20:31.629227 21464 deprecation_wrapper.py:119] From D:\\Home\\Software\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the signation and why is the top of the same questionable and conditions within the class of a company of the best polls of tank to the story of the dead cards and smoke on a color programmer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "\n",
    "textgen = textgenrnn()\n",
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can supply a prefix to prime the model with text to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When life gives you lemons and a badge of the monable of his boys.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"When life gives you lemons \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also let the model try different \"temperatures\". The \"temperature\" controls the level of random choice when picking the next character, instead of always the most likely one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "I have no matter to work on a stranger and I don't know what to do with the streamer and should be a good day. I have a programming but I have a second thing that you would like to be a good day?\n",
      "\n",
      "[PS4] [H] 100 GREATE - Some Power [W] Paypal\n",
      "\n",
      "I got a good day of the same to my favorite parents in the money in the same series in the partner who are a bank of the rest of the same time to see the background of the world of the first time in a bank of the story of the state of the bank of a stream.\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "Did anyone else put my friends and a song with generation?\n",
      "\n",
      "A power from a real lifting car change of this song in showing the market and something in the same time in the Protonspoint of the New York the one of the most super a world is online with the new week. Here's a stray about the state of the same thing you can be bad that I saw this subreddit in th\n",
      "\n",
      "Fallout 4 - One of the experience of the banner in the different seattle of the best day of the same time that is a computer of my girlfriend who really made it some advice and the first time in the story.\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "Full of Twitter - Feels;d Steam Scores on Red Headline, Walk May Can Free Years That You've just planned same. So not rea, but I just finished my dad using a go for your wife?\n",
      "\n",
      "The James Marchine Unknown Campaign\n",
      "\n",
      "Anybody know how to afrightless\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try different prefixes and temperatures. (examine the `.generate()` function, by running a cell with `textgenrnn.generate?`)\n",
    "* Try a different pretrained model from `textgenrnn`\n",
    "* Advanced: train your own model! `textgenrnn` provide a **very** simple mechanism to do so: https://github.com/minimaxir/textgenrnn#examples, you just need to supply a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that were here. My light, but won't start with me.. d2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"I have a dream that\", temperature=1.0)  # temperature can be [1.0, 0.5, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that she was a community planet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"I have a dream that\", temperature=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream that would be a good day!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"I have a dream that\", temperature=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somebody once told me the world was a lot of the world at the way to start this subreddit and something that you guys would be a high programming section and was in the same time to the world on the most part of the same time to see the program and I have a streamer in the state of the state of the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"Somebody once told me the world was \", temperature=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somebody once told me the world was writing the ball because the oil night\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"Somebody once told me the world was \", temperature=1.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying a different pretrained model from textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgen2 = textgenrnn(weights_path=\"./textgenrnnWeights/reddit_legaladvice_relationshipadvice.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somebody once told me the world was cheating about him bad it not willing to false dden back me and doesn't paint me bend on pain for cited. She was there.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen2.generate(prefix=\"Somebody once told me the world was \", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgen3 = textgenrnn(weights_path=\"./textgenrnnWeights/hacker_news.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somebody once told me the world was the web\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen3.generate(prefix=\"Somebody once told me the world was \", temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are relative newcomers to the language processing world. They are an evolution of recurrent neural networks and activation layers. Using transformers has increased the capability of generating believable text by a whole lot, so much so that [ethical issues](https://www.theverge.com/2019/2/21/18234500/ai-ethics-debate-researchers-harmful-programs-openai) have arised around release of models or restrictive use of them.\n",
    "\n",
    "Architecture wise, transformers are an encoder-decoder scheme that relies heavily on \"attention\" - a mechanism that allows every step to examine both past and future.\n",
    "\n",
    "<img src=\"http://lilianweng.github.io/lil-log/assets/images/transformer.png\" />\n",
    "\n",
    "One recent model from OpenAI is GPT-2, which is freely available for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your user gives you lemons you generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers\\exampes\\run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"When life gives you lemons\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py --model_type=gpt2 --length=200 --model_name_or_path=gpt2 --stop_token=\".\" --prompt=\"When life gives you lemons\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      "\n",
      "And suddenly there  was a powerful tidal wave coming out of her ears.\n",
      "\n",
      "'Hey..'\n",
      "\n",
      "Professor McGonagall shook her head with a glance at Goyle,\n",
      "\n",
      "'Woof! Don't...I'm getting...shit! Shit!'\n",
      "\n",
      "Goyle was covered in flaming hair\n",
      "\n",
      "'Thank god I didn't say anything about it!'\n",
      "\n",
      "Now there was blood flowing from Professor McGonagall's face as she saw. Something yellow oozed out of her nose.\n",
      "\n",
      "Faint red vomit appeared from the spot.\n",
      "\n",
      "'Guzzle! Can you smell...!'\n",
      "\n",
      "Instantly other students quickly followed in dark circles around the teleportation weapon.\n",
      "\n",
      "Instantly darker objects appeared.\n",
      "\n",
      "One of the red objects looked like a shinier one or two scuttlecheeks.\n",
      "\n",
      "The red color drained from Goyle's face.\n",
      "\n",
      "Goyle let out a terror without smiling.\n",
      "\n",
      "But,\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=200 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --prompt=\"Harry witnessed Professor McGonagall walking right past Peeves who \\\n",
    "was determinedly loosening a crystal chandelier and could have sworn he heard her \\\n",
    "tell the poltergeist out of the corner of her mouth, 'It unscrews the other way.’\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's see how it does with Williams' \"This is just to say\" (https://poets.org/poem/just-say) poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and are later sliced off, for you and those you love to share with others\n"
     ]
    }
   ],
   "source": [
    "!python ./transformers/examples/run_generation.py \\\n",
    "    --model_type=gpt2 \\\n",
    "    --length=50 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --stop_token=\".\" \\\n",
    "    --prompt=\"I have eaten the plums \\\n",
    "that were in the icebox \\\n",
    "and which you were probably \\\n",
    "saving for breakfast\" 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this BTW is one of my favorite poems ever. So sweet and so plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try different prefix inputs\n",
    "* Try different temperatures with the `--temperature` argument.\n",
    "* Advanced: Try a different model than GPT-2.\n",
    "\n",
    "On the help section for the generation script you can find all the models:\n",
    "```\n",
    "--model_name_or_path MODEL_NAME_OR_PATH\n",
    "                        Path to pre-trained model or shortcut name selected in\n",
    "                        the list: gpt2, gpt2-medium, gpt2-large, distilgpt2,\n",
    "                        openai-gpt, xlnet-base-cased, xlnet-large-cased,\n",
    "                        transfo-xl-wt103, xlm-mlm-en-2048, xlm-mlm-ende-1024,\n",
    "                        xlm-mlm-enfr-1024, xlm-mlm-enro-1024, xlm-mlm-tlm-\n",
    "                        xnli15-1024, xlm-mlm-xnli15-1024, xlm-clm-enfr-1024,\n",
    "                        xlm-clm-ende-1024, xlm-mlm-17-1280, xlm-mlm-100-1280,\n",
    "                        ctrl\n",
    "```\n",
    "The `ctrl` model is very recent work (from SalesForce research), just from a couple of weeks ago, it's supposed to be really awesome at controling the output text. Be warned - the model is a **6Gb download**! It might be worth it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatBot\n",
    "\n",
    "[Chatbots](https://en.wikipedia.org/wiki/Chatbot) are conversational AI agents that can respond to text input. It's still ways away from a convincing conversation in general open-ended scenarios, but in certain applications chatbots are a big success, e.g. in the public services industry's online portals.\n",
    "\n",
    "`huggingface` again have released their pretrained models for chatbots based on transformers just a few months ago: https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313#79c5\n",
    "\n",
    "You can also use their online demo: https://convai.huggingface.co/persona/my-only-friend-is-a-dog-i-work-at-a-newspaper-my-father-used-to-be-a-butcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.7/site-packages (1.2.0)\n",
      "Collecting pytorch-ignite\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/31/efcc2b587419b1f54c5c6ef51996f91bb5d8f760537d17de674c89e06048/pytorch_ignite-0.2.1-py2.py3-none-any.whl (84kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 2.4MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (4.36.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (0.0.35)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (1.17.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (2.22.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (0.1.83)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (1.3.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (2019.8.19)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (from pytorch_transformers) (1.9.253)\n",
      "Requirement already satisfied: six in /usr/local/Cellar/protobuf/3.10.0/libexec/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (0.14.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->pytorch_transformers) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->pytorch_transformers) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->pytorch_transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->pytorch_transformers) (2.8)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.253 in /usr/local/lib/python3.7/site-packages (from boto3->pytorch_transformers) (1.12.253)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.9.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.253->boto3->pytorch_transformers) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.253->boto3->pytorch_transformers) (0.15.2)\n",
      "Installing collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pytorch_transformers pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transfer-learning-conv-ai'...\n",
      "remote: Enumerating objects: 87, done.\u001b[K\n",
      "remote: Total 87 (delta 0), reused 0 (delta 0), pack-reused 87\u001b[K\n",
      "Unpacking objects: 100% (87/87), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transfer-learning-conv-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,threading,subprocess,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_proc():\n",
    "    proc = subprocess.Popen([sys.executable, \n",
    "                             os.getcwd()+'/transfer-learning-conv-ai/interact.py'\n",
    "                            ],\n",
    "                            stdout=subprocess.PIPE,\n",
    "                            stdin=subprocess.PIPE,\n",
    "                            stderr=subprocess.DEVNULL)\n",
    "    pout = proc.stdout\n",
    "    pin = proc.stdin\n",
    "    \n",
    "    return proc,pout,pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_proc, cb1_pout, cb1_pin = chatbot_proc(); # create a chatbot process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_pin.write(b\"--temperature=1.1\\n\"), cb1_pin.flush();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> hi how are you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk to your chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i'm doing well just listening to some music\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cb1_pin.write(b\"i'm doing mighty fine! and how are you?\\n\"), cb1_pin.flush();\n",
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i am listening to a lot of pop music\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cb1_pin.write(b\"no way! i'm also listening to music. what music are you listening to?\\n\"), cb1_pin.flush();\n",
    "print(cb1_pout.readline().decode(sys.stdout.encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also quite funny to get it to talk to itself - it never get tired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_output = b\"i am listening to a lot of pop music\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: yeah, i know what you mean.\n",
      "A: what do you do for a living?\n",
      "B: i am a mechanic.\n",
      "A: i am a pilot.\n",
      "B: what do you do for work?\n",
      "A: i fix planes.\n",
      "B: what kind of planes do you have?\n",
      "A: do you have any hobbies?\n",
      "B: i like to listen to music.\n",
      "A: what kind of music do you like?\n"
     ]
    }
   ],
   "source": [
    "partyA = True\n",
    "for _ in range(10):\n",
    "    partyA = not partyA\n",
    "    cb1_pin.write(cb1_output), cb1_pin.flush();\n",
    "    cb1_output = cb1_pout.readline()[4:]\n",
    "    print(\"%s: %s\" % ('A' if partyA else 'B',\n",
    "          cb1_output[:-1].decode(sys.stdout.encoding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb1_proc.kill() # kill the chatbot process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try some different inputs\n",
    "* Advanced: Spin up another chatbot and have them talk to one another (by feeding the outputs across)\n",
    "* Advanced: Use a different underlying model than GPT-2 for your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "That's a wrap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
